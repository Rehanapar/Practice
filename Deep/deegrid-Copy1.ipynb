{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cee1e23-9419-4de1-8f5e-4e199894de93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   id              1 non-null      int64 \n",
      " 1   original_text   1 non-null      object\n",
      " 2   rewrite_prompt  1 non-null      object\n",
      " 3   rewritten_text  1 non-null      object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 164.0+ bytes\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'model'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m categorical_columns:\n\u001b[0;32m     23\u001b[0m     label_encoders[col] \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[1;32m---> 24\u001b[0m     train_df[col] \u001b[38;5;241m=\u001b[39m label_encoders[col]\u001b[38;5;241m.\u001b[39mfit_transform(train_df[col])\n\u001b[0;32m     25\u001b[0m     test_df[col] \u001b[38;5;241m=\u001b[39m test_df[col]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: label_encoders[col]\u001b[38;5;241m.\u001b[39mtransform([x])[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m label_encoders[col]\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Function to convert running values from 'km' to 'miles'\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'model'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "# Load train and test datasets\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Checking information of the train data\n",
    "train_df.info()\n",
    "\n",
    "# Initialize LabelEncoder and dictionary to store encoders for each column\n",
    "label_encoders = {}\n",
    "\n",
    "# List of categorical columns to encode\n",
    "categorical_columns = ['model', 'motor_type', 'wheel', 'color', 'status', 'type']\n",
    "\n",
    "# Apply LabelEncoder to each categorical column for both train and test\n",
    "for col in categorical_columns:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    train_df[col] = label_encoders[col].fit_transform(train_df[col])\n",
    "    test_df[col] = test_df[col].apply(lambda x: label_encoders[col].transform([x])[0] if x in label_encoders[col].classes_ else -1)\n",
    "\n",
    "# Function to convert running values from 'km' to 'miles'\n",
    "def convert_running(value):\n",
    "    if 'km' in value:\n",
    "        kilometers = int(value.split()[0])\n",
    "        miles = kilometers * 0.621371\n",
    "        return f\"{miles:.2f} miles\"\n",
    "    elif 'miles' in value:\n",
    "        return value\n",
    "    else:\n",
    "        return value \n",
    "\n",
    "# Apply the conversion for both train and test datasets\n",
    "train_df['running'] = train_df['running'].apply(convert_running)\n",
    "test_df['running'] = test_df['running'].apply(convert_running)\n",
    "\n",
    "# Clean and convert 'running' column to numeric values\n",
    "train_df['running'] = train_df['running'].str.replace('miles', '').str.strip()\n",
    "test_df['running'] = test_df['running'].str.replace('miles', '').str.strip()\n",
    "train_df['running'] = pd.to_numeric(train_df['running'], errors='coerce').fillna(0).astype('int64')\n",
    "test_df['running'] = pd.to_numeric(test_df['running'], errors='coerce').fillna(0).astype('int64')\n",
    "\n",
    "# Drop duplicates from the training data\n",
    "train_df = train_df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3c570e83-7a03-4aaa-b65f-7865db17d0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1642 entries, 0 to 1641\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   model         1642 non-null   int32  \n",
      " 1   year          1642 non-null   int64  \n",
      " 2   motor_type    1642 non-null   int32  \n",
      " 3   running       1642 non-null   int64  \n",
      " 4   wheel         1642 non-null   int32  \n",
      " 5   color         1642 non-null   int32  \n",
      " 6   type          1642 non-null   int32  \n",
      " 7   status        1642 non-null   int32  \n",
      " 8   motor_volume  1642 non-null   float64\n",
      " 9   price         1642 non-null   int64  \n",
      "dtypes: float64(1), int32(6), int64(3)\n",
      "memory usage: 89.9 KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74642ba2-79eb-4dc9-a223-14daa9a167c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['price', 'wheel'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare features and target variable\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X \u001b[38;5;241m=\u001b[39m train_df\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwheel\u001b[39m\u001b[38;5;124m'\u001b[39m], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Replace 'target_column' with your actual target column name\u001b[39;00m\n\u001b[0;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5582\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5583\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5584\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5585\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5586\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5587\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5588\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5589\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['price', 'wheel'] not found in axis\""
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare features and target variable\n",
    "X = train_df.drop(['price', 'wheel'], axis = 1) # Replace 'target_column' with your actual target column name\n",
    "y = train_df['price']  # Your target variable\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fe928ab8-09b1-420b-9ed3-5e2205fde17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16714316-3a1c-48d2-8075-6167cd7c79f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\rehana\\anaconda3\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from keras) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from keras) (0.13.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from keras) (0.4.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from keras) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from optree->keras) (4.11.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c72a6d73-a315-4569-860a-50efa768f49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\rehana\\anaconda3\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8aa62cfb-6f1e-4212-a9f1-6bd3328a4fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikeras[tensorflow-cpu]\n",
      "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from scikeras[tensorflow-cpu]) (3.6.0)\n",
      "Requirement already satisfied: scikit-learn>=1.4.2 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from scikeras[tensorflow-cpu]) (1.4.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from keras>=3.2.0->scikeras[tensorflow-cpu]) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from keras>=3.2.0->scikeras[tensorflow-cpu]) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from keras>=3.2.0->scikeras[tensorflow-cpu]) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from keras>=3.2.0->scikeras[tensorflow-cpu]) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from keras>=3.2.0->scikeras[tensorflow-cpu]) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from keras>=3.2.0->scikeras[tensorflow-cpu]) (0.13.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from keras>=3.2.0->scikeras[tensorflow-cpu]) (0.4.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from keras>=3.2.0->scikeras[tensorflow-cpu]) (23.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras[tensorflow-cpu]) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras[tensorflow-cpu]) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras[tensorflow-cpu]) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from optree->keras>=3.2.0->scikeras[tensorflow-cpu]) (4.11.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->scikeras[tensorflow-cpu]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->scikeras[tensorflow-cpu]) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\rehana\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->scikeras[tensorflow-cpu]) (0.1.0)\n",
      "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: scikeras\n",
      "Successfully installed scikeras-0.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)'))': /simple/scikeras/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)'))': /simple/scikeras/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)'))': /simple/scikeras/\n",
      "WARNING: scikeras 0.13.0 does not provide the extra 'tensorflow-cpu'\n"
     ]
    }
   ],
   "source": [
    "pip install scikeras[tensorflow-cpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "767598ba-6a4a-4b47-a4d0-f0fd353a854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7bb114e-c5b7-4f33-ae69-4a5e2d22c491",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras.wrappers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscikit_learn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasRegressor\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.wrappers'"
     ]
    }
   ],
   "source": [
    "\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "78ae9220-f1ca-4156-810a-525c370fb092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1306, 8), (327, 8), (1306,), (327,))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_temp.shape, y_train.shape, y_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "eede831c-9d48-40ca-bdc9-6f2afb687a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_val, y_test, y_val = train_test_split( X_temp, X_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d701026a-7007-442e-94c6-c11e73f0502b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((163, 8), (164, 8), (163, 8), (164, 8))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, X_val.shape, y_test.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "33f09d99-8905-49fb-b1ac-c0b41ebc849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_temp_scaled = scaler.transform(X_temp)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_scaled = scaler.transform(test_df.drop(columns=['Id', 'wheel']))  # Adjust as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "37018ba3-dce2-4616-a603-f7b1c77cc484",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_scaled = scaler.transform(test_df.drop(columns=['Id', 'wheel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23137488-b293-4650-b607-c46152e86a64",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras.wrappers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscikit_learn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasRegressor\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.wrappers'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0b6ccccd-11e5-410d-a347-539fca964fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rehana\\anaconda3\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "C:\\Users\\Rehana\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.24610183085594875 using {'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras import layers, models\n",
    " \n",
    "def create_model(learning_rate=0.01):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    " \n",
    "model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=10, learning_rate=0.001, verbose=0)\n",
    "# `KerasRegressor(learning_rate=0.001)\n",
    "param_grid = {'learning_rate': [0.001, 0.01, 0.1]}\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, y)\n",
    " \n",
    "print(f'Best: {grid_result.best_score_} using {grid_result.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0f66d59d-4e69-45e9-a4b1-b110f2432c07",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KerasRegressor' object has no attribute 'compile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[134], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_absolute_error\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KerasRegressor' object has no attribute 'compile'"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.1)\n",
    "model.compile(optimizer=optimizer, loss='mean_absolute_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7fa3b11b-34df-4e80-a038-e069c99d32bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 30 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Rehana\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Rehana\\anaconda3\\Lib\\site-packages\\scikeras\\wrappers.py\", line 770, in fit\n    self._fit(\n  File \"C:\\Users\\Rehana\\anaconda3\\Lib\\site-packages\\scikeras\\wrappers.py\", line 928, in _fit\n    self._ensure_compiled_model()\n  File \"C:\\Users\\Rehana\\anaconda3\\Lib\\site-packages\\scikeras\\wrappers.py\", line 446, in _ensure_compiled_model\n    raise ValueError(\"You must provide a loss or a compiled model\")\nValueError: You must provide a loss or a compiled model\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[140], line 23\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# `KerasRegressor(learning_rate=0.001)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# param_grid = {'batch_size': [10, 20, 40],\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#     'epochs': [50, 100],\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#     'learning_rate': [0.001, 0.01, 0.1]}\u001b[39;00m\n\u001b[0;32m     20\u001b[0m grid \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel, param_distributions\u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m40\u001b[39m],\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m],\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m]}, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m grid_result \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_result\u001b[38;5;241m.\u001b[39mbest_score_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_result\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1914\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1914\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1915\u001b[0m         ParameterSampler(\n\u001b[0;32m   1916\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_distributions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state\n\u001b[0;32m   1917\u001b[0m         )\n\u001b[0;32m   1918\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:947\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    942\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    943\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    944\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    945\u001b[0m     )\n\u001b[1;32m--> 947\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_score)\n\u001b[0;32m    949\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:536\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    530\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    531\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    535\u001b[0m     )\n\u001b[1;32m--> 536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    539\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    540\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 30 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Rehana\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Rehana\\anaconda3\\Lib\\site-packages\\scikeras\\wrappers.py\", line 770, in fit\n    self._fit(\n  File \"C:\\Users\\Rehana\\anaconda3\\Lib\\site-packages\\scikeras\\wrappers.py\", line 928, in _fit\n    self._ensure_compiled_model()\n  File \"C:\\Users\\Rehana\\anaconda3\\Lib\\site-packages\\scikeras\\wrappers.py\", line 446, in _ensure_compiled_model\n    raise ValueError(\"You must provide a loss or a compiled model\")\nValueError: You must provide a loss or a compiled model\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras import layers, models\n",
    " \n",
    "def create_model(learning_rate=0.01):\n",
    "    model = keras.Sequential([\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=(X.shape[1],)),\n",
    "    # layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    # layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    layers.Dense(1)])  # Output layer for regression\n",
    "    return model\n",
    " \n",
    "model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=10, learning_rate=0.001, verbose=0)\n",
    "# `KerasRegressor(learning_rate=0.001)\n",
    "# param_grid = {'batch_size': [10, 20, 40],\n",
    "#     'epochs': [50, 100],\n",
    "#     'learning_rate': [0.001, 0.01, 0.1]}\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions= {'batch_size': [10, 20, 40],\n",
    "    'epochs': [50, 100],\n",
    "    'learning_rate': [0.001, 0.01, 0.1]}, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, y)\n",
    " \n",
    "print(f'Best: {grid_result.best_score_} using {grid_result.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ec28d343-b6b4-45bd-b9bb-1b2244294172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rehana\\anaconda3\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "C:\\Users\\Rehana\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.46881758714277044 using {'learning_rate': 0.001, 'epochs': 100, 'batch_size': 10}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Function to create the model\n",
    "def create_model(learning_rate=0.01):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, activation='relu', input_shape=(X.shape[1],)))  # Ensure correct input shape\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1))  # Output layer\n",
    "    model.compile(optimizer='adam', loss='mae')  # Ensure model is compiled with loss\n",
    "    return model\n",
    "\n",
    "# Create KerasRegressor\n",
    "model = KerasRegressor(build_fn=create_model, learning_rate=0.01, verbose=0)\n",
    "\n",
    "# Define the parameter distributions\n",
    "param_distributions = {\n",
    "    'batch_size': [10, 20, 40],\n",
    "    'epochs': [50, 100],\n",
    "    'learning_rate': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Create the RandomizedSearchCV object\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_distributions, n_jobs=-1, cv=3, n_iter=10)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(f'Best: {grid_result.best_score_} using {grid_result.best_params_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57654ff9-be85-404d-a012-29910c63f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the model\n",
    "model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Define the parameter distributions\n",
    "param_distributions = {\n",
    "    'batch_size': [10, 20, 40],\n",
    "    'epochs': [50, 100],\n",
    "    'learning_rate': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Create the RandomizedSearchCV object\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_distributions, n_jobs=-1, cv=3, n_iter=10)\n",
    "grid_result = grid.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(f'Best: {grid_result.best_score_} using {grid_result.best_params_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e6b7d543-abea-4b63-8a72-58728f935c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter learning_rate for estimator KerasRegressor.\nThis issue can likely be resolved by setting this parameter in the KerasRegressor constructor:\n`KerasRegressor(learning_rate=0.1)`\nCheck the list of available parameters with `estimator.get_params().keys()`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[122], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m KerasRegressor(build_fn\u001b[38;5;241m=\u001b[39mmodel, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     10\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel, param_distributions\u001b[38;5;241m=\u001b[39mparam_distributions, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m random_search_result \u001b[38;5;241m=\u001b[39m random_search\u001b[38;5;241m.\u001b[39mfit(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1914\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1914\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1915\u001b[0m         ParameterSampler(\n\u001b[0;32m   1916\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_distributions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state\n\u001b[0;32m   1917\u001b[0m         )\n\u001b[0;32m   1918\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    917\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    918\u001b[0m         clone(base_estimator),\n\u001b[0;32m    919\u001b[0m         X,\n\u001b[0;32m    920\u001b[0m         y,\n\u001b[0;32m    921\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    922\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    923\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    924\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    925\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    927\u001b[0m     )\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    929\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    930\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    931\u001b[0m     )\n\u001b[0;32m    932\u001b[0m )\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:883\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    876\u001b[0m score_params_test \u001b[38;5;241m=\u001b[39m _check_method_params(X, params\u001b[38;5;241m=\u001b[39mscore_params, indices\u001b[38;5;241m=\u001b[39mtest)\n\u001b[0;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parameters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;66;03m# here we clone the parameters, since sometimes the parameters\u001b[39;00m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;66;03m# themselves might be estimators, e.g. when we search over different\u001b[39;00m\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;66;03m# estimators in a pipeline.\u001b[39;00m\n\u001b[0;32m    882\u001b[0m     \u001b[38;5;66;03m# ref: https://github.com/scikit-learn/scikit-learn/pull/26786\u001b[39;00m\n\u001b[1;32m--> 883\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclone(parameters, safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m    885\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    887\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, train)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scikeras\\wrappers.py:1175\u001b[0m, in \u001b[0;36mBaseWrapper.set_params\u001b[1;34m(self, **params)\u001b[0m\n\u001b[0;32m   1171\u001b[0m             \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{param: value})\n\u001b[0;32m   1172\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m             \u001b[38;5;66;03m# Give a SciKeras specific user message to aid\u001b[39;00m\n\u001b[0;32m   1174\u001b[0m             \u001b[38;5;66;03m# in moving from the Keras wrappers\u001b[39;00m\n\u001b[1;32m-> 1175\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1176\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for estimator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1177\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThis issue can likely be resolved by setting this parameter\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1178\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m constructor:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1179\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1180\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCheck the list of available parameters with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1181\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `estimator.get_params().keys()`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1182\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter learning_rate for estimator KerasRegressor.\nThis issue can likely be resolved by setting this parameter in the KerasRegressor constructor:\n`KerasRegressor(learning_rate=0.1)`\nCheck the list of available parameters with `estimator.get_params().keys()`"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    " \n",
    "param_distributions = {\n",
    "    'batch_size': [10, 20, 40],\n",
    "    'epochs': [50, 100],\n",
    "    'learning_rate': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "model = KerasRegressor(build_fn=model, epochs=100, batch_size=10, verbose=0)\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_distributions, n_iter=5, cv=3, verbose=1)\n",
    "random_search_result = random_search.fit(X, y)\n",
    " \n",
    "# print(f'Best: {random_search_result.best_score_} using {random_search_result.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c31d4976-48b2-49d4-bb9a-8c57e99db79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "906d26bf-05e1-46d9-97f6-fdca041251f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rehana\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the deep learning model with improved architecture\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=(X.shape[1],)),\n",
    "    # layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    # layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    layers.Dense(1)  # Output layer for regression\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "74fc88da-625d-4375-a75a-203936469467",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the model with a learning rate scheduler\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mean_absolute_error', metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "741001d1-af15-4250-b76f-b567c548a9c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KerasRegressor' object has no attribute 'compile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[152], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_absolute_error\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KerasRegressor' object has no attribute 'compile'"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mean_absolute_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "154ed765-201e-438c-aad6-172f8294bce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Fit the model with early stopping\n",
    "# early_stopping = keras.callbacks.EarlyStopping(monitor='val_mae', patience=10, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "57444a11-448d-4a55-9d75-3f2f00ab3a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 15790.1768 - mae: 15786.6963\n",
      "Epoch 2/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3995.0793 - mae: 3972.5393\n",
      "Epoch 3/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3286.3457 - mae: 3263.3560\n",
      "Epoch 4/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2978.4805 - mae: 2955.5396\n",
      "Epoch 5/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3022.5305 - mae: 2999.7568\n",
      "Epoch 6/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3008.3120 - mae: 2985.6716\n",
      "Epoch 7/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2882.6902 - mae: 2860.2097\n",
      "Epoch 8/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2820.1829 - mae: 2797.7700\n",
      "Epoch 9/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3073.9973 - mae: 3051.7065\n",
      "Epoch 10/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2887.0151 - mae: 2864.8765\n",
      "Epoch 11/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2941.8281 - mae: 2919.7166\n",
      "Epoch 12/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3066.0535 - mae: 3043.8604\n",
      "Epoch 13/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2833.9495 - mae: 2811.9658\n",
      "Epoch 14/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2907.9023 - mae: 2885.9231\n",
      "Epoch 15/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2833.3372 - mae: 2811.3086\n",
      "Epoch 16/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2669.4851 - mae: 2647.6267\n",
      "Epoch 17/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2763.6841 - mae: 2741.7908\n",
      "Epoch 18/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3001.6099 - mae: 2979.7915\n",
      "Epoch 19/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2801.2090 - mae: 2779.3403\n",
      "Epoch 20/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2833.3435 - mae: 2811.4709\n",
      "Epoch 21/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2710.5979 - mae: 2688.7356\n",
      "Epoch 22/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2687.6743 - mae: 2665.8728\n",
      "Epoch 23/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2843.3096 - mae: 2821.4744\n",
      "Epoch 24/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2946.6355 - mae: 2924.7261\n",
      "Epoch 25/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2971.2505 - mae: 2949.2920\n",
      "Epoch 26/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2865.7695 - mae: 2843.8870\n",
      "Epoch 27/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2678.7026 - mae: 2656.8032\n",
      "Epoch 28/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2890.7253 - mae: 2868.5496\n",
      "Epoch 29/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2717.3162 - mae: 2695.1707\n",
      "Epoch 30/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2647.7515 - mae: 2625.6658\n",
      "Epoch 31/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2884.5955 - mae: 2862.3391\n",
      "Epoch 32/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2932.5671 - mae: 2910.2671\n",
      "Epoch 33/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2767.2859 - mae: 2744.8726\n",
      "Epoch 34/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2706.3955 - mae: 2683.8633\n",
      "Epoch 35/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2816.6443 - mae: 2794.0168\n",
      "Epoch 36/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2694.5752 - mae: 2671.8477\n",
      "Epoch 37/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2713.6582 - mae: 2690.7354\n",
      "Epoch 38/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2723.8069 - mae: 2700.8779\n",
      "Epoch 39/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2717.8364 - mae: 2694.6738\n",
      "Epoch 40/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2632.4543 - mae: 2609.1733\n",
      "Epoch 41/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2728.4075 - mae: 2705.0815\n",
      "Epoch 42/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2589.3330 - mae: 2565.9021\n",
      "Epoch 43/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2702.1318 - mae: 2678.5625\n",
      "Epoch 44/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2890.6863 - mae: 2866.8684\n",
      "Epoch 45/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2566.0359 - mae: 2542.2205\n",
      "Epoch 46/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2628.5720 - mae: 2604.5684\n",
      "Epoch 47/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2609.4072 - mae: 2585.1960\n",
      "Epoch 48/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2746.6416 - mae: 2722.2507\n",
      "Epoch 49/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2696.3291 - mae: 2671.7839\n",
      "Epoch 50/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2640.0840 - mae: 2615.2979\n",
      "Epoch 51/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2580.1748 - mae: 2555.2339\n",
      "Epoch 52/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2738.7424 - mae: 2713.7129\n",
      "Epoch 53/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2577.0522 - mae: 2551.8730\n",
      "Epoch 54/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2757.2156 - mae: 2731.9104\n",
      "Epoch 55/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2629.9697 - mae: 2604.4419\n",
      "Epoch 56/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2642.0273 - mae: 2616.3376\n",
      "Epoch 57/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2788.1396 - mae: 2762.2913\n",
      "Epoch 58/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2506.3638 - mae: 2480.3369\n",
      "Epoch 59/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2536.2620 - mae: 2509.9719\n",
      "Epoch 60/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2622.5845 - mae: 2596.1482\n",
      "Epoch 61/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2688.2295 - mae: 2661.6890\n",
      "Epoch 62/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2465.8745 - mae: 2439.1826\n",
      "Epoch 63/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2490.6338 - mae: 2463.7156\n",
      "Epoch 64/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2719.8159 - mae: 2692.8091\n",
      "Epoch 65/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2529.0305 - mae: 2501.7927\n",
      "Epoch 66/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2588.3523 - mae: 2561.1133\n",
      "Epoch 67/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2512.9084 - mae: 2485.3484\n",
      "Epoch 68/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2471.2544 - mae: 2443.5808\n",
      "Epoch 69/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2493.9556 - mae: 2466.2200\n",
      "Epoch 70/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2437.1072 - mae: 2409.1682\n",
      "Epoch 71/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2470.7390 - mae: 2442.6223\n",
      "Epoch 72/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2514.7246 - mae: 2486.4917\n",
      "Epoch 73/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2723.4006 - mae: 2694.8674\n",
      "Epoch 74/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2542.9900 - mae: 2514.3450\n",
      "Epoch 75/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2485.4739 - mae: 2456.6370\n",
      "Epoch 76/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2413.6255 - mae: 2384.5952\n",
      "Epoch 77/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2400.4688 - mae: 2371.2778\n",
      "Epoch 78/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2425.2539 - mae: 2395.8750\n",
      "Epoch 79/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2459.8379 - mae: 2430.2141\n",
      "Epoch 80/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2362.5527 - mae: 2332.8577\n",
      "Epoch 81/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2311.7683 - mae: 2281.8396\n",
      "Epoch 82/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2422.5706 - mae: 2392.4612\n",
      "Epoch 83/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2343.4631 - mae: 2313.2771\n",
      "Epoch 84/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2334.9231 - mae: 2304.4548\n",
      "Epoch 85/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2311.2864 - mae: 2280.6074\n",
      "Epoch 86/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2327.5688 - mae: 2296.6423\n",
      "Epoch 87/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2355.6099 - mae: 2324.5723\n",
      "Epoch 88/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2199.3645 - mae: 2168.0574\n",
      "Epoch 89/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2264.0273 - mae: 2232.5154\n",
      "Epoch 90/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2218.3171 - mae: 2186.6851\n",
      "Epoch 91/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2299.9158 - mae: 2268.0547\n",
      "Epoch 92/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2242.2485 - mae: 2210.1040\n",
      "Epoch 93/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2349.7568 - mae: 2317.3765\n",
      "Epoch 94/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2232.9685 - mae: 2200.3252\n",
      "Epoch 95/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2224.4106 - mae: 2191.5950\n",
      "Epoch 96/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2184.8669 - mae: 2151.7671\n",
      "Epoch 97/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2176.2522 - mae: 2142.9595\n",
      "Epoch 98/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2169.5645 - mae: 2135.9900\n",
      "Epoch 99/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2139.3547 - mae: 2105.6018\n",
      "Epoch 100/100\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2198.0117 - mae: 2164.0298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e63f03c080>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y,  epochs = 100, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5d43835e-b4cd-4456-ae60-cbacfb1352e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 8129.1431 - mae: 8104.6016\n",
      "Epoch 2/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3731.0955 - mae: 3691.5864\n",
      "Epoch 3/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3575.1614 - mae: 3537.0552\n",
      "Epoch 4/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3404.4873 - mae: 3370.9392\n",
      "Epoch 5/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3533.2515 - mae: 3500.7925\n",
      "Epoch 6/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3759.2654 - mae: 3728.6338\n",
      "Epoch 7/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3829.9690 - mae: 3801.7749\n",
      "Epoch 8/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3356.8799 - mae: 3328.5056\n",
      "Epoch 9/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2962.0908 - mae: 2932.6965\n",
      "Epoch 10/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3300.5544 - mae: 3268.2996\n",
      "Epoch 11/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3185.2717 - mae: 3153.9819\n",
      "Epoch 12/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3268.5796 - mae: 3235.8811\n",
      "Epoch 13/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3057.7056 - mae: 3023.8257\n",
      "Epoch 14/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3116.3979 - mae: 3081.8381\n",
      "Epoch 15/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3257.8887 - mae: 3224.2297\n",
      "Epoch 16/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2794.9436 - mae: 2759.2383\n",
      "Epoch 17/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3538.6548 - mae: 3502.4355\n",
      "Epoch 18/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3159.3179 - mae: 3122.3494\n",
      "Epoch 19/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3009.5862 - mae: 2970.2964\n",
      "Epoch 20/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2916.5615 - mae: 2877.0095\n",
      "Epoch 21/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2792.1951 - mae: 2752.2358\n",
      "Epoch 22/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2733.4524 - mae: 2692.1135\n",
      "Epoch 23/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2977.9404 - mae: 2935.4609\n",
      "Epoch 24/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2829.1738 - mae: 2784.9346\n",
      "Epoch 25/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2914.1172 - mae: 2869.4902\n",
      "Epoch 26/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2796.7607 - mae: 2752.5728\n",
      "Epoch 27/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2969.4358 - mae: 2925.3323\n",
      "Epoch 28/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2916.3643 - mae: 2869.6609\n",
      "Epoch 29/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2962.3567 - mae: 2914.9714\n",
      "Epoch 30/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3075.4509 - mae: 3027.8306\n",
      "Epoch 31/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2774.8706 - mae: 2727.8628\n",
      "Epoch 32/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2868.7732 - mae: 2818.1565\n",
      "Epoch 33/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2833.9468 - mae: 2778.9346\n",
      "Epoch 34/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2683.9722 - mae: 2627.0400\n",
      "Epoch 35/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2678.4912 - mae: 2623.0359\n",
      "Epoch 36/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2612.8303 - mae: 2554.1946\n",
      "Epoch 37/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2935.3176 - mae: 2875.9377\n",
      "Epoch 38/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2889.1506 - mae: 2826.0586\n",
      "Epoch 39/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2687.9788 - mae: 2617.6455\n",
      "Epoch 40/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2640.0093 - mae: 2559.0828\n",
      "Epoch 41/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2478.0964 - mae: 2384.9299\n",
      "Epoch 42/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2378.0220 - mae: 2268.2876\n",
      "Epoch 43/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2581.3601 - mae: 2466.7498\n",
      "Epoch 44/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2106.3508 - mae: 1977.7035\n",
      "Epoch 45/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2192.6721 - mae: 2056.1401\n",
      "Epoch 46/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2295.3909 - mae: 2164.4709\n",
      "Epoch 47/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2335.7004 - mae: 2208.5847\n",
      "Epoch 48/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2240.3643 - mae: 2115.4785\n",
      "Epoch 49/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2206.4492 - mae: 2085.8308\n",
      "Epoch 50/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2144.8772 - mae: 2024.6868\n",
      "Epoch 51/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2492.3657 - mae: 2367.9785\n",
      "Epoch 52/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2159.8440 - mae: 2041.0844\n",
      "Epoch 53/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2115.5403 - mae: 1999.0831\n",
      "Epoch 54/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2312.8787 - mae: 2202.5613\n",
      "Epoch 55/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2213.6255 - mae: 2103.5527\n",
      "Epoch 56/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2190.7837 - mae: 2063.9346\n",
      "Epoch 57/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2234.5100 - mae: 2102.7195\n",
      "Epoch 58/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2166.2366 - mae: 2046.6624\n",
      "Epoch 59/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2252.4568 - mae: 2138.1345\n",
      "Epoch 60/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2292.5181 - mae: 2181.3777\n",
      "Epoch 61/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2150.9089 - mae: 2040.3370\n",
      "Epoch 62/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2173.9907 - mae: 2061.5352\n",
      "Epoch 63/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2083.4231 - mae: 1975.4673\n",
      "Epoch 64/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2136.2859 - mae: 2027.6819\n",
      "Epoch 65/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2096.8577 - mae: 1987.0640\n",
      "Epoch 66/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2134.3423 - mae: 2024.9049\n",
      "Epoch 67/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2192.9902 - mae: 2087.2898\n",
      "Epoch 68/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2103.3945 - mae: 1989.9323\n",
      "Epoch 69/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2185.2419 - mae: 2073.8030\n",
      "Epoch 70/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2138.7444 - mae: 2028.6442\n",
      "Epoch 71/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2167.9363 - mae: 2059.9304\n",
      "Epoch 72/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2369.8716 - mae: 2262.5425\n",
      "Epoch 73/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2232.6016 - mae: 2127.2134\n",
      "Epoch 74/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2209.5759 - mae: 2104.8167\n",
      "Epoch 75/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2041.6184 - mae: 1929.3419\n",
      "Epoch 76/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2201.3076 - mae: 2092.8342\n",
      "Epoch 77/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1990.8940 - mae: 1885.1091\n",
      "Epoch 78/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2172.0410 - mae: 2062.7012\n",
      "Epoch 79/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1970.1248 - mae: 1859.9812\n",
      "Epoch 80/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2059.5193 - mae: 1950.1263\n",
      "Epoch 81/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2134.5447 - mae: 2025.8955\n",
      "Epoch 82/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2222.7407 - mae: 2110.6211\n",
      "Epoch 83/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2262.9412 - mae: 2148.9187\n",
      "Epoch 84/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1967.1273 - mae: 1855.9619\n",
      "Epoch 85/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2086.1023 - mae: 1977.8164\n",
      "Epoch 86/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1953.5493 - mae: 1847.0276\n",
      "Epoch 87/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1996.0356 - mae: 1892.5193\n",
      "Epoch 88/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1939.2765 - mae: 1833.9291\n",
      "Epoch 89/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2111.0322 - mae: 2007.6241\n",
      "Epoch 90/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2235.9067 - mae: 2131.9128\n",
      "Epoch 91/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2090.5740 - mae: 1985.3320\n",
      "Epoch 92/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1997.1700 - mae: 1888.6011\n",
      "Epoch 93/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2105.7905 - mae: 1998.9594\n",
      "Epoch 94/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1911.8713 - mae: 1808.5891\n",
      "Epoch 95/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2141.7432 - mae: 2036.2542\n",
      "Epoch 96/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2090.0647 - mae: 1985.1227\n",
      "Epoch 97/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2111.4138 - mae: 2005.9675\n",
      "Epoch 98/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2001.3906 - mae: 1898.0459\n",
      "Epoch 99/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2097.7480 - mae: 1996.3341\n",
      "Epoch 100/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2102.1001 - mae: 1999.4954\n",
      "Epoch 101/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2131.0579 - mae: 2027.1755\n",
      "Epoch 102/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1989.2836 - mae: 1887.0054\n",
      "Epoch 103/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2259.4858 - mae: 2157.3606\n",
      "Epoch 104/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2036.2001 - mae: 1930.9547\n",
      "Epoch 105/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1974.8652 - mae: 1871.4547\n",
      "Epoch 106/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2079.3806 - mae: 1977.5477\n",
      "Epoch 107/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2098.7649 - mae: 1999.1238\n",
      "Epoch 108/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2038.6260 - mae: 1939.7631\n",
      "Epoch 109/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1949.3574 - mae: 1851.7758\n",
      "Epoch 110/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1878.9307 - mae: 1780.7212\n",
      "Epoch 111/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2113.1809 - mae: 2015.5612\n",
      "Epoch 112/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1982.0317 - mae: 1881.7778\n",
      "Epoch 113/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2043.3354 - mae: 1943.2788\n",
      "Epoch 114/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2105.1616 - mae: 2006.9316\n",
      "Epoch 115/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2068.6709 - mae: 1970.6655\n",
      "Epoch 116/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1986.9863 - mae: 1886.9835\n",
      "Epoch 117/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2061.7878 - mae: 1959.1212\n",
      "Epoch 118/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2100.9927 - mae: 1999.2252\n",
      "Epoch 119/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1953.5179 - mae: 1851.0645\n",
      "Epoch 120/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1993.7466 - mae: 1891.4706\n",
      "Epoch 121/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2086.6514 - mae: 1985.8883\n",
      "Epoch 122/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2226.1228 - mae: 2125.7256\n",
      "Epoch 123/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1962.7556 - mae: 1862.8354\n",
      "Epoch 124/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1980.9142 - mae: 1880.8723\n",
      "Epoch 125/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1929.3065 - mae: 1829.0089\n",
      "Epoch 126/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2035.6213 - mae: 1933.7454\n",
      "Epoch 127/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2082.7795 - mae: 1982.1572\n",
      "Epoch 128/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2035.0676 - mae: 1933.3195\n",
      "Epoch 129/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1915.8549 - mae: 1814.0846\n",
      "Epoch 130/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1946.7565 - mae: 1844.0087\n",
      "Epoch 131/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1877.7625 - mae: 1773.9757\n",
      "Epoch 132/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2073.1208 - mae: 1972.8586\n",
      "Epoch 133/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2022.0758 - mae: 1923.6166\n",
      "Epoch 134/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2057.7126 - mae: 1956.9779\n",
      "Epoch 135/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1987.6896 - mae: 1888.2872\n",
      "Epoch 136/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2058.5430 - mae: 1959.0259\n",
      "Epoch 137/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2008.0249 - mae: 1905.4813\n",
      "Epoch 138/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1971.2791 - mae: 1869.4515\n",
      "Epoch 139/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2061.9177 - mae: 1962.6013\n",
      "Epoch 140/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1920.7883 - mae: 1821.0299\n",
      "Epoch 141/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2002.3123 - mae: 1903.2139\n",
      "Epoch 142/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1986.0408 - mae: 1888.5341\n",
      "Epoch 143/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2041.0634 - mae: 1944.5844\n",
      "Epoch 144/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1849.2384 - mae: 1752.6298\n",
      "Epoch 145/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1939.3649 - mae: 1841.6445\n",
      "Epoch 146/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2096.2473 - mae: 1997.4207\n",
      "Epoch 147/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1964.4659 - mae: 1863.8750\n",
      "Epoch 148/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1919.7192 - mae: 1820.0272\n",
      "Epoch 149/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2040.9894 - mae: 1942.0990\n",
      "Epoch 150/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2008.7969 - mae: 1911.2883\n",
      "Epoch 151/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1919.8810 - mae: 1823.0671\n",
      "Epoch 152/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1932.2107 - mae: 1835.1581\n",
      "Epoch 153/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1946.8866 - mae: 1849.7118\n",
      "Epoch 154/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2044.6906 - mae: 1948.1975\n",
      "Epoch 155/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1933.1677 - mae: 1835.3779\n",
      "Epoch 156/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1971.2446 - mae: 1871.4042\n",
      "Epoch 157/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1906.9656 - mae: 1809.1405\n",
      "Epoch 158/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1987.5933 - mae: 1889.1296\n",
      "Epoch 159/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1937.6152 - mae: 1840.4609\n",
      "Epoch 160/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1973.8154 - mae: 1876.6799\n",
      "Epoch 161/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1859.2213 - mae: 1763.7107\n",
      "Epoch 162/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2022.0198 - mae: 1926.2676\n",
      "Epoch 163/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2076.7080 - mae: 1980.0852\n",
      "Epoch 164/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1943.8990 - mae: 1846.1783\n",
      "Epoch 165/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1990.9053 - mae: 1893.1873\n",
      "Epoch 166/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1995.7740 - mae: 1898.5974\n",
      "Epoch 167/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2100.1526 - mae: 2003.2681\n",
      "Epoch 168/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2124.0857 - mae: 2025.7814\n",
      "Epoch 169/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1863.8827 - mae: 1767.5239\n",
      "Epoch 170/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1864.6511 - mae: 1766.9849\n",
      "Epoch 171/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1953.1095 - mae: 1855.1013\n",
      "Epoch 172/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2008.2167 - mae: 1908.4749\n",
      "Epoch 173/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1906.8824 - mae: 1808.6907\n",
      "Epoch 174/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1875.1230 - mae: 1778.0205\n",
      "Epoch 175/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2064.4353 - mae: 1967.1384\n",
      "Epoch 176/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1962.7423 - mae: 1866.6227\n",
      "Epoch 177/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1946.8674 - mae: 1849.3773\n",
      "Epoch 178/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1877.8154 - mae: 1780.5706\n",
      "Epoch 179/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1962.6477 - mae: 1866.3412\n",
      "Epoch 180/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1884.8311 - mae: 1788.2362\n",
      "Epoch 181/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1935.1017 - mae: 1838.0787\n",
      "Epoch 182/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1931.3643 - mae: 1836.9683\n",
      "Epoch 183/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1855.4434 - mae: 1760.6940\n",
      "Epoch 184/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1809.6691 - mae: 1713.1919\n",
      "Epoch 185/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1975.3563 - mae: 1878.8521\n",
      "Epoch 186/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2110.5969 - mae: 2013.8519\n",
      "Epoch 187/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2019.5059 - mae: 1924.5699\n",
      "Epoch 188/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2021.0074 - mae: 1923.6136\n",
      "Epoch 189/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2139.5142 - mae: 2042.3499\n",
      "Epoch 190/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2087.6099 - mae: 1991.0518\n",
      "Epoch 191/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1863.4360 - mae: 1767.4827\n",
      "Epoch 192/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1814.6139 - mae: 1719.9977\n",
      "Epoch 193/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1981.0491 - mae: 1885.7383\n",
      "Epoch 194/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1883.4789 - mae: 1790.6583\n",
      "Epoch 195/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2023.2863 - mae: 1929.7064\n",
      "Epoch 196/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2011.1378 - mae: 1917.6758\n",
      "Epoch 197/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1936.4930 - mae: 1843.3959\n",
      "Epoch 198/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1859.4385 - mae: 1766.3352\n",
      "Epoch 199/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1962.4332 - mae: 1868.7798\n",
      "Epoch 200/200\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1920.1841 - mae: 1825.5640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e63a536ea0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y,\n",
    "                    epochs=200, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "371aab15-1f06-41df-943c-1bd071f6d40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 15909.5254 - mae: 15906.9775 - val_loss: 9574.1611 - val_mae: 9571.6182\n",
      "Epoch 2/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 15959.2031 - mae: 15956.2773 - val_loss: 9860.4951 - val_mae: 9855.4814\n",
      "Epoch 3/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15102.1104 - mae: 15095.7373 - val_loss: 11736.2207 - val_mae: 11724.7109\n",
      "Epoch 4/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11280.0928 - mae: 11266.3828 - val_loss: 18136.1836 - val_mae: 18114.5742\n",
      "Epoch 5/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4507.8594 - mae: 4484.5669 - val_loss: 20060.3516 - val_mae: 20036.4375\n",
      "Epoch 6/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3637.7019 - mae: 3613.7273 - val_loss: 19988.3887 - val_mae: 19964.5527\n",
      "Epoch 7/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3312.1526 - mae: 3288.0740 - val_loss: 20375.8828 - val_mae: 20351.5742\n",
      "Epoch 8/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3296.3596 - mae: 3271.9343 - val_loss: 20159.9746 - val_mae: 20135.8613\n",
      "Epoch 9/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3190.1792 - mae: 3165.9453 - val_loss: 20151.1719 - val_mae: 20127.0078\n",
      "Epoch 10/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3085.5405 - mae: 3061.3093 - val_loss: 20465.0625 - val_mae: 20440.5273\n",
      "Epoch 11/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3220.5178 - mae: 3196.1167 - val_loss: 20391.1836 - val_mae: 20366.7305\n",
      "Epoch 12/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2929.8071 - mae: 2905.2334 - val_loss: 20206.5918 - val_mae: 20182.3242\n",
      "Epoch 13/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3029.2002 - mae: 3004.9246 - val_loss: 20442.0859 - val_mae: 20417.5723\n",
      "Epoch 14/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2957.3438 - mae: 2932.7935 - val_loss: 20366.7070 - val_mae: 20342.2773\n",
      "Epoch 15/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3091.1328 - mae: 3066.6748 - val_loss: 20312.9766 - val_mae: 20288.5977\n",
      "Epoch 16/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3193.5369 - mae: 3169.0430 - val_loss: 20623.6445 - val_mae: 20598.9395\n",
      "Epoch 17/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2960.1760 - mae: 2935.6704 - val_loss: 20347.7930 - val_mae: 20323.4199\n",
      "Epoch 18/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2976.2534 - mae: 2951.8481 - val_loss: 20371.1230 - val_mae: 20346.7344\n",
      "Epoch 19/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3089.8613 - mae: 3065.3428 - val_loss: 20343.1230 - val_mae: 20318.7637\n",
      "Epoch 20/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3285.7351 - mae: 3261.2166 - val_loss: 20171.4766 - val_mae: 20147.3027\n",
      "Epoch 21/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2825.7219 - mae: 2801.2961 - val_loss: 20504.5879 - val_mae: 20480.0938\n",
      "Epoch 22/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2859.3594 - mae: 2834.8936 - val_loss: 20397.0234 - val_mae: 20372.6211\n",
      "Epoch 23/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2918.2358 - mae: 2893.8271 - val_loss: 20532.1543 - val_mae: 20507.5996\n",
      "Epoch 24/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3062.6670 - mae: 3038.1604 - val_loss: 20506.3594 - val_mae: 20481.8496\n",
      "Epoch 25/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2813.9844 - mae: 2789.4614 - val_loss: 20404.1777 - val_mae: 20379.7539\n",
      "Epoch 26/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2773.9604 - mae: 2749.4956 - val_loss: 20586.0234 - val_mae: 20561.4141\n",
      "Epoch 27/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2780.1023 - mae: 2755.5754 - val_loss: 20567.8633 - val_mae: 20543.2676\n",
      "Epoch 28/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2868.5615 - mae: 2844.0186 - val_loss: 20376.3691 - val_mae: 20351.9590\n",
      "Epoch 29/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2773.7610 - mae: 2749.2107 - val_loss: 20478.4004 - val_mae: 20453.8867\n",
      "Epoch 30/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2752.9836 - mae: 2728.5312 - val_loss: 20639.3691 - val_mae: 20614.6934\n",
      "Epoch 31/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2785.3267 - mae: 2760.7847 - val_loss: 20530.3926 - val_mae: 20505.7949\n",
      "Epoch 32/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2852.1707 - mae: 2827.6294 - val_loss: 20603.3125 - val_mae: 20578.6348\n",
      "Epoch 33/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3099.3152 - mae: 3074.6868 - val_loss: 20435.7012 - val_mae: 20411.1777\n",
      "Epoch 34/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2924.3621 - mae: 2899.7712 - val_loss: 20535.6582 - val_mae: 20511.0312\n",
      "Epoch 35/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2881.8206 - mae: 2857.1514 - val_loss: 20369.1133 - val_mae: 20344.6348\n",
      "Epoch 36/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3043.7551 - mae: 3019.0984 - val_loss: 20283.2832 - val_mae: 20258.8359\n",
      "Epoch 37/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3127.3997 - mae: 3102.8171 - val_loss: 20384.5801 - val_mae: 20360.0488\n",
      "Epoch 38/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2668.3545 - mae: 2643.6606 - val_loss: 20544.7266 - val_mae: 20520.0059\n",
      "Epoch 39/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2891.5076 - mae: 2866.8149 - val_loss: 20531.3398 - val_mae: 20506.5977\n",
      "Epoch 40/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2942.8608 - mae: 2918.1675 - val_loss: 20444.6465 - val_mae: 20419.9629\n",
      "Epoch 41/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2728.4900 - mae: 2703.7415 - val_loss: 20648.1523 - val_mae: 20623.2676\n",
      "Epoch 42/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2871.1069 - mae: 2846.3586 - val_loss: 20509.4551 - val_mae: 20484.6895\n",
      "Epoch 43/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2805.3950 - mae: 2780.6482 - val_loss: 20399.8809 - val_mae: 20375.1504\n",
      "Epoch 44/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2775.5845 - mae: 2750.7930 - val_loss: 20665.6797 - val_mae: 20640.6914\n",
      "Epoch 45/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2861.3062 - mae: 2836.4329 - val_loss: 20444.9688 - val_mae: 20420.1660\n",
      "Epoch 46/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3024.4109 - mae: 2999.5442 - val_loss: 20476.1211 - val_mae: 20451.2441\n",
      "Epoch 47/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2934.7793 - mae: 2909.8884 - val_loss: 20395.9141 - val_mae: 20371.0801\n",
      "Epoch 48/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3097.3943 - mae: 3072.4084 - val_loss: 20580.5742 - val_mae: 20555.5645\n",
      "Epoch 49/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2923.0164 - mae: 2898.0771 - val_loss: 20610.4453 - val_mae: 20585.3574\n",
      "Epoch 50/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2921.1128 - mae: 2896.0713 - val_loss: 20460.7109 - val_mae: 20435.7285\n",
      "Epoch 51/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2874.7937 - mae: 2849.7417 - val_loss: 20647.0820 - val_mae: 20621.9141\n",
      "Epoch 52/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2805.3486 - mae: 2780.2200 - val_loss: 20459.8125 - val_mae: 20434.7402\n",
      "Epoch 53/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2815.8408 - mae: 2790.7175 - val_loss: 20336.7598 - val_mae: 20311.7754\n",
      "Epoch 54/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3018.6025 - mae: 2993.4548 - val_loss: 20549.7148 - val_mae: 20524.4980\n",
      "Epoch 55/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2651.8054 - mae: 2626.6279 - val_loss: 20374.8125 - val_mae: 20349.7031\n",
      "Epoch 56/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2729.8904 - mae: 2704.7705 - val_loss: 20667.7402 - val_mae: 20642.3164\n",
      "Epoch 57/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2742.2935 - mae: 2716.8955 - val_loss: 20698.8457 - val_mae: 20673.3672\n",
      "Epoch 58/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2570.4014 - mae: 2545.0898 - val_loss: 20396.5527 - val_mae: 20371.2949\n",
      "Epoch 59/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2903.7671 - mae: 2878.3960 - val_loss: 20334.8574 - val_mae: 20309.6309\n",
      "Epoch 60/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2847.3513 - mae: 2822.0186 - val_loss: 20675.7246 - val_mae: 20650.1250\n",
      "Epoch 61/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2872.3157 - mae: 2846.7744 - val_loss: 20290.9102 - val_mae: 20265.6094\n",
      "Epoch 62/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2787.6602 - mae: 2762.1675 - val_loss: 20744.2188 - val_mae: 20718.4277\n",
      "Epoch 63/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2881.9392 - mae: 2856.2310 - val_loss: 20629.8906 - val_mae: 20604.1699\n",
      "Epoch 64/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2881.7126 - mae: 2856.0322 - val_loss: 20554.3789 - val_mae: 20528.6875\n",
      "Epoch 65/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2838.1487 - mae: 2812.4263 - val_loss: 20520.7109 - val_mae: 20494.9688\n",
      "Epoch 66/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2817.7109 - mae: 2791.9719 - val_loss: 20701.3223 - val_mae: 20675.3828\n",
      "Epoch 67/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3054.2253 - mae: 3028.3552 - val_loss: 20496.5391 - val_mae: 20470.6699\n",
      "Epoch 68/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2755.1665 - mae: 2729.2537 - val_loss: 20755.8574 - val_mae: 20729.7285\n",
      "Epoch 69/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2832.4146 - mae: 2806.3933 - val_loss: 20539.8633 - val_mae: 20513.8516\n",
      "Epoch 70/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2684.5903 - mae: 2658.6074 - val_loss: 20785.9316 - val_mae: 20759.6758\n",
      "Epoch 71/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2591.7473 - mae: 2565.6226 - val_loss: 20493.3789 - val_mae: 20467.2930\n",
      "Epoch 72/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2960.6750 - mae: 2934.5728 - val_loss: 20695.9434 - val_mae: 20669.6582\n",
      "Epoch 73/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2869.8391 - mae: 2843.5540 - val_loss: 20615.4766 - val_mae: 20589.1836\n",
      "Epoch 74/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2701.8701 - mae: 2675.6313 - val_loss: 20435.2168 - val_mae: 20409.0312\n",
      "Epoch 75/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2744.1309 - mae: 2717.8521 - val_loss: 20534.8594 - val_mae: 20508.4688\n",
      "Epoch 76/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2635.7056 - mae: 2609.2939 - val_loss: 20531.3379 - val_mae: 20504.9062\n",
      "Epoch 77/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2724.9421 - mae: 2698.4302 - val_loss: 20265.7734 - val_mae: 20239.5254\n",
      "Epoch 78/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2776.0396 - mae: 2749.5579 - val_loss: 20427.7402 - val_mae: 20401.2637\n",
      "Epoch 79/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2874.4917 - mae: 2847.9382 - val_loss: 20699.5742 - val_mae: 20672.7539\n",
      "Epoch 80/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2713.9106 - mae: 2687.1628 - val_loss: 20504.7168 - val_mae: 20478.0020\n",
      "Epoch 81/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2642.3174 - mae: 2615.5774 - val_loss: 20462.5234 - val_mae: 20435.7578\n",
      "Epoch 82/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2730.0537 - mae: 2703.2136 - val_loss: 20541.4238 - val_mae: 20514.5312\n",
      "Epoch 83/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2668.1619 - mae: 2641.2234 - val_loss: 20516.6328 - val_mae: 20489.7031\n",
      "Epoch 84/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2708.3628 - mae: 2681.4250 - val_loss: 20663.1133 - val_mae: 20635.9551\n",
      "Epoch 85/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2739.0920 - mae: 2712.0220 - val_loss: 20570.1699 - val_mae: 20543.0098\n",
      "Epoch 86/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2834.0283 - mae: 2806.8962 - val_loss: 20495.1934 - val_mae: 20468.0137\n",
      "Epoch 87/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2541.8479 - mae: 2514.6626 - val_loss: 20505.8477 - val_mae: 20478.5801\n",
      "Epoch 88/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2688.1865 - mae: 2660.9717 - val_loss: 20422.1973 - val_mae: 20394.9648\n",
      "Epoch 89/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2854.9546 - mae: 2827.5735 - val_loss: 20423.3242 - val_mae: 20395.9570\n",
      "Epoch 90/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2789.4722 - mae: 2761.9878 - val_loss: 20134.3750 - val_mae: 20107.1680\n",
      "Epoch 91/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2618.5247 - mae: 2591.1074 - val_loss: 20621.0410 - val_mae: 20593.4004\n",
      "Epoch 92/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2736.7126 - mae: 2709.0049 - val_loss: 20511.8965 - val_mae: 20484.2090\n",
      "Epoch 93/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2688.3987 - mae: 2660.6851 - val_loss: 20623.6309 - val_mae: 20595.7930\n",
      "Epoch 94/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2615.2300 - mae: 2587.4194 - val_loss: 20747.4492 - val_mae: 20719.4492\n",
      "Epoch 95/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2720.7334 - mae: 2692.7842 - val_loss: 20576.1641 - val_mae: 20548.2070\n",
      "Epoch 96/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2770.6587 - mae: 2742.6140 - val_loss: 20548.6328 - val_mae: 20520.6016\n",
      "Epoch 97/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2707.0378 - mae: 2678.9099 - val_loss: 20657.2383 - val_mae: 20629.0234\n",
      "Epoch 98/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2732.0481 - mae: 2703.8127 - val_loss: 20493.4336 - val_mae: 20465.2715\n",
      "Epoch 99/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2708.3892 - mae: 2680.1477 - val_loss: 20725.1484 - val_mae: 20696.6875\n",
      "Epoch 100/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2737.8567 - mae: 2709.4998 - val_loss: 20578.6875 - val_mae: 20550.2422\n",
      "Epoch 101/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2689.5691 - mae: 2661.1292 - val_loss: 20859.0117 - val_mae: 20830.2715\n",
      "Epoch 102/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2749.2913 - mae: 2720.6775 - val_loss: 20465.8711 - val_mae: 20437.3281\n",
      "Epoch 103/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2574.0129 - mae: 2545.3958 - val_loss: 20533.9941 - val_mae: 20505.3223\n",
      "Epoch 104/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2612.5713 - mae: 2583.8977 - val_loss: 20515.3086 - val_mae: 20486.5234\n",
      "Epoch 105/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2566.4131 - mae: 2537.6045 - val_loss: 20752.2715 - val_mae: 20723.2188\n",
      "Epoch 106/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2707.4272 - mae: 2678.4104 - val_loss: 20495.9395 - val_mae: 20466.9688\n",
      "Epoch 107/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2497.0061 - mae: 2467.9565 - val_loss: 20367.0859 - val_mae: 20338.0938\n",
      "Epoch 108/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2734.1292 - mae: 2705.0403 - val_loss: 20236.1992 - val_mae: 20207.2168\n",
      "Epoch 109/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2694.0239 - mae: 2664.8354 - val_loss: 20481.5898 - val_mae: 20452.3359\n",
      "Epoch 110/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2676.6558 - mae: 2647.3469 - val_loss: 20735.1816 - val_mae: 20705.6172\n",
      "Epoch 111/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2564.0437 - mae: 2534.5229 - val_loss: 20682.5586 - val_mae: 20652.9688\n",
      "Epoch 112/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2552.0845 - mae: 2522.5298 - val_loss: 20523.5117 - val_mae: 20493.8789\n",
      "Epoch 113/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2635.1868 - mae: 2605.5142 - val_loss: 20601.6562 - val_mae: 20571.8535\n",
      "Epoch 114/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2773.1633 - mae: 2743.3215 - val_loss: 20571.3223 - val_mae: 20541.4570\n",
      "Epoch 115/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2633.5071 - mae: 2603.5520 - val_loss: 20535.1250 - val_mae: 20505.1699\n",
      "Epoch 116/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2681.4399 - mae: 2651.4014 - val_loss: 20589.4668 - val_mae: 20559.3828\n",
      "Epoch 117/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2684.6487 - mae: 2654.5605 - val_loss: 20712.3965 - val_mae: 20682.0781\n",
      "Epoch 118/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2634.0459 - mae: 2603.7432 - val_loss: 20694.4336 - val_mae: 20664.0059\n",
      "Epoch 119/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2721.8062 - mae: 2691.4365 - val_loss: 20525.0996 - val_mae: 20494.6777\n",
      "Epoch 120/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2523.2908 - mae: 2492.8254 - val_loss: 20529.5586 - val_mae: 20498.9902\n",
      "Epoch 121/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2560.8984 - mae: 2530.2246 - val_loss: 20610.5840 - val_mae: 20579.9043\n",
      "Epoch 122/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2769.2947 - mae: 2738.5046 - val_loss: 20532.2656 - val_mae: 20501.5059\n",
      "Epoch 123/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2804.8064 - mae: 2774.0049 - val_loss: 20418.9062 - val_mae: 20388.0527\n",
      "Epoch 124/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2530.6191 - mae: 2499.6482 - val_loss: 20516.9902 - val_mae: 20485.9961\n",
      "Epoch 125/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2764.7920 - mae: 2733.6919 - val_loss: 20613.4824 - val_mae: 20582.2754\n",
      "Epoch 126/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2553.3923 - mae: 2522.1196 - val_loss: 20367.2402 - val_mae: 20336.1465\n",
      "Epoch 127/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2505.9526 - mae: 2474.7217 - val_loss: 20460.6230 - val_mae: 20429.3418\n",
      "Epoch 128/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2646.7227 - mae: 2615.3359 - val_loss: 20531.9824 - val_mae: 20500.4961\n",
      "Epoch 129/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2713.1279 - mae: 2681.5994 - val_loss: 20567.0879 - val_mae: 20535.4941\n",
      "Epoch 130/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2520.3853 - mae: 2488.7578 - val_loss: 20582.3555 - val_mae: 20550.6660\n",
      "Epoch 131/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2508.8450 - mae: 2477.1328 - val_loss: 20729.0059 - val_mae: 20697.0898\n",
      "Epoch 132/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2581.4270 - mae: 2549.5127 - val_loss: 20329.2012 - val_mae: 20297.4062\n",
      "Epoch 133/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2509.8083 - mae: 2477.8513 - val_loss: 20753.7188 - val_mae: 20721.5723\n",
      "Epoch 134/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2487.1680 - mae: 2455.0720 - val_loss: 20502.2617 - val_mae: 20470.1934\n",
      "Epoch 135/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2557.2808 - mae: 2525.0706 - val_loss: 20730.4551 - val_mae: 20698.1230\n",
      "Epoch 136/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2696.0308 - mae: 2663.6992 - val_loss: 20567.1719 - val_mae: 20534.8711\n",
      "Epoch 137/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2601.4890 - mae: 2569.1187 - val_loss: 20576.1230 - val_mae: 20543.6367\n",
      "Epoch 138/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2554.3391 - mae: 2521.8003 - val_loss: 20685.5586 - val_mae: 20652.9180\n",
      "Epoch 139/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2503.2803 - mae: 2470.6138 - val_loss: 20433.2832 - val_mae: 20400.7051\n",
      "Epoch 140/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2781.5413 - mae: 2748.7432 - val_loss: 20493.2285 - val_mae: 20460.5234\n",
      "Epoch 141/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2578.6086 - mae: 2545.8301 - val_loss: 20728.0000 - val_mae: 20695.0273\n",
      "Epoch 142/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2518.6851 - mae: 2485.7275 - val_loss: 20694.7559 - val_mae: 20661.6934\n",
      "Epoch 143/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2685.7239 - mae: 2652.6150 - val_loss: 21002.8906 - val_mae: 20969.5605\n",
      "Epoch 144/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2422.7371 - mae: 2389.5461 - val_loss: 20585.1836 - val_mae: 20552.0098\n",
      "Epoch 145/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2449.0532 - mae: 2415.8418 - val_loss: 20675.9883 - val_mae: 20642.6523\n",
      "Epoch 146/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2405.2256 - mae: 2371.8809 - val_loss: 20539.8789 - val_mae: 20506.5215\n",
      "Epoch 147/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2452.8357 - mae: 2419.3550 - val_loss: 20525.5176 - val_mae: 20492.0762\n",
      "Epoch 148/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2444.0947 - mae: 2410.5562 - val_loss: 20475.3164 - val_mae: 20441.8496\n",
      "Epoch 149/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2419.4756 - mae: 2385.8853 - val_loss: 20615.1191 - val_mae: 20581.4355\n",
      "Epoch 150/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2626.8889 - mae: 2593.1904 - val_loss: 20793.9824 - val_mae: 20760.0938\n",
      "Epoch 151/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2675.8564 - mae: 2642.0046 - val_loss: 20848.1953 - val_mae: 20814.1836\n",
      "Epoch 152/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2460.8232 - mae: 2426.8633 - val_loss: 20740.2637 - val_mae: 20706.2285\n",
      "Epoch 153/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2354.0427 - mae: 2319.9841 - val_loss: 20742.1973 - val_mae: 20708.0176\n",
      "Epoch 154/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2538.5552 - mae: 2504.3730 - val_loss: 20512.4199 - val_mae: 20478.2676\n",
      "Epoch 155/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2721.1702 - mae: 2686.8787 - val_loss: 20815.5684 - val_mae: 20781.1582\n",
      "Epoch 156/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2440.6548 - mae: 2406.2354 - val_loss: 20599.1309 - val_mae: 20564.7539\n",
      "Epoch 157/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2631.8352 - mae: 2597.4182 - val_loss: 20799.8105 - val_mae: 20765.2227\n",
      "Epoch 158/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2390.0229 - mae: 2355.4839 - val_loss: 20762.8516 - val_mae: 20728.1504\n",
      "Epoch 159/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2416.9707 - mae: 2382.2778 - val_loss: 20596.3105 - val_mae: 20561.5938\n",
      "Epoch 160/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2286.2556 - mae: 2251.4919 - val_loss: 20874.8691 - val_mae: 20839.9238\n",
      "Epoch 161/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2357.5195 - mae: 2322.6296 - val_loss: 20768.9180 - val_mae: 20733.9160\n",
      "Epoch 162/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2427.9377 - mae: 2392.9253 - val_loss: 20573.1738 - val_mae: 20538.1816\n",
      "Epoch 163/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2361.7942 - mae: 2326.7412 - val_loss: 21045.2188 - val_mae: 21009.8691\n",
      "Epoch 164/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2406.0120 - mae: 2370.7434 - val_loss: 20767.6484 - val_mae: 20732.3359\n",
      "Epoch 165/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2336.9722 - mae: 2301.6685 - val_loss: 20841.8906 - val_mae: 20806.3984\n",
      "Epoch 166/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2425.7246 - mae: 2390.3032 - val_loss: 20961.9609 - val_mae: 20926.3320\n",
      "Epoch 167/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2349.0657 - mae: 2313.4768 - val_loss: 20806.1973 - val_mae: 20770.5391\n",
      "Epoch 168/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2382.2014 - mae: 2346.5315 - val_loss: 20846.7949 - val_mae: 20810.9805\n",
      "Epoch 169/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2525.4739 - mae: 2489.6868 - val_loss: 20793.6973 - val_mae: 20757.7949\n",
      "Epoch 170/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2441.8613 - mae: 2405.9497 - val_loss: 20636.5859 - val_mae: 20600.7109\n",
      "Epoch 171/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2364.7148 - mae: 2328.7893 - val_loss: 20792.6289 - val_mae: 20756.5449\n",
      "Epoch 172/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2388.3748 - mae: 2352.3252 - val_loss: 20934.7109 - val_mae: 20898.3867\n",
      "Epoch 173/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2290.7004 - mae: 2254.3848 - val_loss: 20841.3711 - val_mae: 20805.0391\n",
      "Epoch 174/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2314.5957 - mae: 2278.2832 - val_loss: 20803.7754 - val_mae: 20767.3145\n",
      "Epoch 175/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2431.4272 - mae: 2394.9536 - val_loss: 20775.4824 - val_mae: 20738.9297\n",
      "Epoch 176/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2342.1375 - mae: 2305.5913 - val_loss: 21120.2559 - val_mae: 21083.4414\n",
      "Epoch 177/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2399.9072 - mae: 2363.1790 - val_loss: 20557.0723 - val_mae: 20520.3867\n",
      "Epoch 178/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2398.0183 - mae: 2361.2417 - val_loss: 20844.6523 - val_mae: 20807.7402\n",
      "Epoch 179/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2287.1172 - mae: 2250.2065 - val_loss: 20916.0977 - val_mae: 20879.0527\n",
      "Epoch 180/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2387.3640 - mae: 2350.3479 - val_loss: 21109.4746 - val_mae: 21072.2402\n",
      "Epoch 181/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2227.9319 - mae: 2190.7883 - val_loss: 20892.0918 - val_mae: 20854.8652\n",
      "Epoch 182/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2442.8755 - mae: 2405.6355 - val_loss: 20748.7695 - val_mae: 20711.5039\n",
      "Epoch 183/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2310.9460 - mae: 2273.6714 - val_loss: 20930.8789 - val_mae: 20893.4121\n",
      "Epoch 184/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2160.3167 - mae: 2122.8574 - val_loss: 20896.7969 - val_mae: 20859.2676\n",
      "Epoch 185/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2147.4285 - mae: 2109.8901 - val_loss: 20803.1973 - val_mae: 20765.5859\n",
      "Epoch 186/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2350.3752 - mae: 2312.7246 - val_loss: 20683.5508 - val_mae: 20645.9062\n",
      "Epoch 187/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2418.1548 - mae: 2380.4553 - val_loss: 20848.0508 - val_mae: 20810.1973\n",
      "Epoch 188/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2135.8403 - mae: 2097.9329 - val_loss: 20827.7754 - val_mae: 20789.8574\n",
      "Epoch 189/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2230.3699 - mae: 2192.4121 - val_loss: 20785.2734 - val_mae: 20747.2930\n",
      "Epoch 190/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2140.6528 - mae: 2102.6274 - val_loss: 20988.9688 - val_mae: 20950.7676\n",
      "Epoch 191/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2245.5557 - mae: 2207.3657 - val_loss: 20784.3965 - val_mae: 20746.1855\n",
      "Epoch 192/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2132.8391 - mae: 2094.6177 - val_loss: 20934.8613 - val_mae: 20896.4688\n",
      "Epoch 193/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2237.4238 - mae: 2199.0049 - val_loss: 20703.9492 - val_mae: 20665.5898\n",
      "Epoch 194/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2294.9402 - mae: 2256.4875 - val_loss: 20828.1289 - val_mae: 20789.6289\n",
      "Epoch 195/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2242.2988 - mae: 2203.7505 - val_loss: 20927.0898 - val_mae: 20888.4160\n",
      "Epoch 196/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2112.2087 - mae: 2073.5410 - val_loss: 20990.9727 - val_mae: 20952.1484\n",
      "Epoch 197/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2360.0432 - mae: 2321.2219 - val_loss: 20964.0039 - val_mae: 20925.0801\n",
      "Epoch 198/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2127.2822 - mae: 2088.3633 - val_loss: 20540.8086 - val_mae: 20501.9941\n",
      "Epoch 199/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2329.8274 - mae: 2290.9041 - val_loss: 20877.1836 - val_mae: 20838.0879\n",
      "Epoch 200/200\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2204.2112 - mae: 2165.0771 - val_loss: 20728.8516 - val_mae: 20689.7344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x171c5355820>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val), \n",
    "                    epochs=200, batch_size=32, verbose=1)\n",
    "\n",
    "# callbacks=[early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "695c820e-68f0-4847-a190-058d105e5ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2146.6504 - mae: 2112.3459\n",
      "Validation Mean Absolute Error: 2099.50\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "val_loss, val_mae = model.evaluate(X, y)\n",
    "print(f'Validation Mean Absolute Error: {val_mae:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cff643a5-0201-4d86-83d9-b0103dfe39f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_val_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the validation set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m val_loss, val_mae \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_val_scaled, y_val)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Mean Absolute Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_mae\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_val_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "val_loss, val_mae = model.evaluate(X_val_scaled, y_val)\n",
    "print(f'Validation Mean Absolute Error: {val_mae:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4fdc16c1-6acc-46fb-83a6-71637cc40214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make predictions on the test set\n",
    "y_predict = model.predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "996cb035-deab-4fd8-a358-e31e77694d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            price\n",
      "0    19677.423828\n",
      "1    17716.781250\n",
      "2    20111.156250\n",
      "3    15289.390625\n",
      "4     8173.140625\n",
      "..            ...\n",
      "406  25007.714844\n",
      "407  13668.597656\n",
      "408  15741.763672\n",
      "409  19339.289062\n",
      "410  13782.019531\n",
      "\n",
      "[411 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame for the predictions\n",
    "df_predictions = pd.DataFrame(y_predict, columns=['price'])\n",
    "\n",
    "# Display predictions\n",
    "print(df_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "cdcf64db-eca3-480b-9961-494e95aac107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19677.423828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17716.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>20111.156250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15289.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8173.140625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>406</td>\n",
       "      <td>25007.714844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>407</td>\n",
       "      <td>13668.597656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>408</td>\n",
       "      <td>15741.763672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>409</td>\n",
       "      <td>19339.289062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>410</td>\n",
       "      <td>13782.019531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id         price\n",
       "0      0  19677.423828\n",
       "1      1  17716.781250\n",
       "2      2  20111.156250\n",
       "3      3  15289.390625\n",
       "4      4   8173.140625\n",
       "..   ...           ...\n",
       "406  406  25007.714844\n",
       "407  407  13668.597656\n",
       "408  408  15741.763672\n",
       "409  409  19339.289062\n",
       "410  410  13782.019531\n",
       "\n",
       "[411 rows x 2 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = pd.concat([test_df['Id'], df_predictions], axis=1)\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b9707188-4fb6-4a3c-896c-dcd0d24a82ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('foo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bbf58a-ed4f-464c-9161-64b55e9c13e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40194487-ace7-4106-baba-60280a20db97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4ee267e8-a002-4ced-8f37-62a93203554c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1642 entries, 0 to 1641\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   model         1642 non-null   object \n",
      " 1   year          1642 non-null   int64  \n",
      " 2   motor_type    1642 non-null   object \n",
      " 3   running       1642 non-null   object \n",
      " 4   wheel         1642 non-null   object \n",
      " 5   color         1642 non-null   object \n",
      " 6   type          1642 non-null   object \n",
      " 7   status        1642 non-null   object \n",
      " 8   motor_volume  1642 non-null   float64\n",
      " 9   price         1642 non-null   int64  \n",
      "dtypes: float64(1), int64(2), object(7)\n",
      "memory usage: 128.4+ KB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from tensorflow.keras import layers, regularizers, Model, Input, optimizers\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "import warnings\n",
    " \n",
    "# Load train and test datasets\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    " \n",
    "# Checking information of the train data\n",
    "train_df.info()\n",
    " \n",
    "# Initialize LabelEncoder and dictionary to store encoders for each column\n",
    "label_encoders = {}\n",
    "categorical_columns = ['model', 'motor_type', 'wheel', 'color', 'status', 'type']\n",
    " \n",
    "# Apply LabelEncoder to each categorical column for both train and test\n",
    "for col in categorical_columns:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    train_df[col] = label_encoders[col].fit_transform(train_df[col])\n",
    "    test_df[col] = test_df[col].apply(lambda x: label_encoders[col].transform([x])[0] if x in label_encoders[col].classes_ else -1)\n",
    " \n",
    "# Function to convert running values from 'km' to 'miles'\n",
    "def convert_running(value):\n",
    "    if 'km' in value:\n",
    "        kilometers = int(value.split()[0])\n",
    "        miles = kilometers * 0.621371\n",
    "        return f\"{miles:.2f} miles\"\n",
    "    elif 'miles' in value:\n",
    "        return value\n",
    "    return value\n",
    " \n",
    "# Apply the conversion for both train and test datasets\n",
    "train_df['running'] = train_df['running'].apply(convert_running)\n",
    "test_df['running'] = test_df['running'].apply(convert_running)\n",
    " \n",
    "# Clean and convert 'running' column to numeric values\n",
    "train_df['running'] = pd.to_numeric(train_df['running'].str.replace('miles', '').str.strip(), errors='coerce').fillna(0).astype('int64')\n",
    "test_df['running'] = pd.to_numeric(test_df['running'].str.replace('miles', '').str.strip(), errors='coerce').fillna(0).astype('int64')\n",
    " \n",
    "# Drop duplicates from the training data\n",
    "train_df = train_df.drop_duplicates()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2982d947-eb5a-4800-bf9d-705458c1f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target variable\n",
    "X = train_df.drop(['price', 'wheel'], axis=1)\n",
    "y = train_df['price']\n",
    " \n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X) # Scale training data\n",
    "X_test_scaled = scaler.transform(test_df.drop(columns=['Id', 'wheel'])) # Scale test data\n",
    " \n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d193785c-bd9c-4ee4-8cc7-0616b2e108aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.6994995276133219 using {'learning_rate': 0.1, 'epochs': 150, 'batch_size': 128}\n",
      "Best parameters found:  {'learning_rate': 0.1, 'epochs': 150, 'batch_size': 128}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to create the Keras model using Functional API\n",
    "def create_model(learning_rate=0.01):\n",
    "    inputs = Input(shape=(X.shape[1],))\n",
    "    x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
    "    x = layers.BatchNormalization()(x) # Add Batch Normalization\n",
    "    x = layers.Dropout(0.3)(x) # Add Dropout to prevent overfitting\n",
    "    x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    outputs = layers.Dense(1)(x) # Output layer for regression\n",
    " \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate) # Use Adam optimizer with learning rate as argument\n",
    "    model.compile(optimizer=optimizer, loss='mae') # Compile with Mean Absolute Error\n",
    "    return model\n",
    " \n",
    "# Create KerasRegressor\n",
    "model = KerasRegressor(build_fn=create_model, verbose=0, learning_rate=0.1)\n",
    " \n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1], # Update with a range of values you want to try\n",
    "    'batch_size': [16, 32, 64, 128],\n",
    "    'epochs': [50, 100, 150]\n",
    "}\n",
    " \n",
    "# Create the RandomizedSearchCV object\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_distributions, n_jobs=-1, cv=3, n_iter=10)\n",
    " \n",
    "# Fit the model\n",
    "grid_result = grid.fit(X_scaled, y)\n",
    " \n",
    "print(f'Best: {grid_result.best_score_} using {grid_result.best_params_}')\n",
    " \n",
    "# Use the best model for predictions\n",
    "best_model = grid_result.best_estimator_\n",
    "best_params = grid.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3869f629-925c-4292-a72a-739dc4150fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.760139548778534 using {'learning_rate': 0.01, 'epochs': 200, 'batch_size': 64}\n",
      "Best parameters found:  {'learning_rate': 0.01, 'epochs': 200, 'batch_size': 64}\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "# Function to create the Keras model using Functional API\n",
    "def create_model(learning_rate=0.01):\n",
    "    inputs = Input(shape=(X.shape[1],))\n",
    "    \n",
    "    # First Dense layer with regularization and activation\n",
    "    x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
    "    x = layers.BatchNormalization()(x)  # Batch Normalization to stabilize training\n",
    "    x = layers.Dropout(0.3)(x)  # Dropout for regularization\n",
    "    \n",
    "    # Second Dense layer with more units\n",
    "    x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)  # Increase Dropout rate\n",
    "    \n",
    "    # Third Dense layer with regularization\n",
    "    x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)  # Increase Dropout rate\n",
    "    \n",
    "    # Output layer for regression\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Using Exponential Decay for learning rate schedule\n",
    "    lr_schedule = ExponentialDecay(initial_learning_rate=learning_rate, decay_steps=10000, decay_rate=0.96)\n",
    "    optimizer = optimizers.Adam(learning_rate=lr_schedule)\n",
    "    \n",
    "    # Compile model with MAE loss\n",
    "    model.compile(optimizer=optimizer, loss='mae')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create KerasRegressor\n",
    "model = KerasRegressor(build_fn=create_model, verbose=0, learning_rate=0.01)\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV with more parameters\n",
    "param_distributions = {\n",
    "    'learning_rate': [0.0001, 0.001, 0.01, 0.1],  # Wider range of learning rates\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [50, 100, 200],  # Explore higher epochs\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV for hyperparameter tuning\n",
    "grid = RandomizedSearchCV(estimator=model, param_distributions=param_distributions, n_jobs=-1, cv=5, n_iter=20)  # Increase n_iter and cv for better search\n",
    "\n",
    "# Callbacks to improve accuracy\n",
    "callbacks = [ReduceLROnPlateau(monitor='loss', factor=0.1, patience=10, min_lr=1e-6)]\n",
    "\n",
    "# Fit the model\n",
    "grid_result = grid.fit(X_scaled, y, callbacks=callbacks)\n",
    "\n",
    "# Get best model and parameters\n",
    "best_model = grid_result.best_estimator_\n",
    "best_params = grid_result.best_params_\n",
    "\n",
    "print(f'Best: {grid_result.best_score_} using {best_params}')\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# Use the best model for predictions\n",
    "y_pred = best_model.predict(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e25d4b79-c981-47fe-887c-cb1b5c01ec1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 16115.4570\n",
      "Epoch 2/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15930.3398\n",
      "Epoch 3/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16045.2578 \n",
      "Epoch 4/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15682.0537\n",
      "Epoch 5/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15719.2939 \n",
      "Epoch 6/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15747.2305 \n",
      "Epoch 7/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15425.9717 \n",
      "Epoch 8/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15209.1855 \n",
      "Epoch 9/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15150.0420 \n",
      "Epoch 10/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14899.4004 \n",
      "Epoch 11/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14801.6094 \n",
      "Epoch 12/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14599.6182\n",
      "Epoch 13/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14522.0547 \n",
      "Epoch 14/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14247.7881 \n",
      "Epoch 15/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13907.4219 \n",
      "Epoch 16/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13674.3555\n",
      "Epoch 17/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13258.4189 \n",
      "Epoch 18/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 12971.7744 \n",
      "Epoch 19/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12241.1104\n",
      "Epoch 20/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11935.7998\n",
      "Epoch 21/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11399.7119 \n",
      "Epoch 22/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11230.9854 \n",
      "Epoch 23/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10686.5947 \n",
      "Epoch 24/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10232.8193 \n",
      "Epoch 25/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9791.1738 \n",
      "Epoch 26/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9441.2529 \n",
      "Epoch 27/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8612.6406 \n",
      "Epoch 28/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8119.8789\n",
      "Epoch 29/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7467.2676 \n",
      "Epoch 30/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6911.3882 \n",
      "Epoch 31/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6409.9473\n",
      "Epoch 32/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5662.7236 \n",
      "Epoch 33/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5208.9570\n",
      "Epoch 34/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4780.2612\n",
      "Epoch 35/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4682.7969 \n",
      "Epoch 36/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3915.6208\n",
      "Epoch 37/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4038.0435\n",
      "Epoch 38/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3588.5530\n",
      "Epoch 39/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3573.7629\n",
      "Epoch 40/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3408.8132\n",
      "Epoch 41/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3376.6982 \n",
      "Epoch 42/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3283.0222 \n",
      "Epoch 43/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3278.1960 \n",
      "Epoch 44/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3153.1812 \n",
      "Epoch 45/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3043.4924 \n",
      "Epoch 46/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3099.2722\n",
      "Epoch 47/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3070.5215 \n",
      "Epoch 48/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3021.7800 \n",
      "Epoch 49/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2938.0208\n",
      "Epoch 50/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2796.0464 \n",
      "Epoch 51/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2847.9338\n",
      "Epoch 52/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2929.6694\n",
      "Epoch 53/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2966.2424\n",
      "Epoch 54/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2800.3811\n",
      "Epoch 55/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2807.4873\n",
      "Epoch 56/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2817.9866 \n",
      "Epoch 57/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2855.1714 \n",
      "Epoch 58/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2588.2356 \n",
      "Epoch 59/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2838.0767\n",
      "Epoch 60/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2878.3533\n",
      "Epoch 61/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2728.6748\n",
      "Epoch 62/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2831.8008\n",
      "Epoch 63/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2734.6931 \n",
      "Epoch 64/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2810.0640 \n",
      "Epoch 65/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2838.8203\n",
      "Epoch 66/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2888.5857\n",
      "Epoch 67/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2847.0310\n",
      "Epoch 68/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2891.8948 \n",
      "Epoch 69/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2700.0659\n",
      "Epoch 70/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2717.5037 \n",
      "Epoch 71/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2827.5952\n",
      "Epoch 72/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2654.9653\n",
      "Epoch 73/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2698.5020 \n",
      "Epoch 74/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2616.8481\n",
      "Epoch 75/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2773.0203\n",
      "Epoch 76/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2664.2954 \n",
      "Epoch 77/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2733.3086\n",
      "Epoch 78/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2789.1782\n",
      "Epoch 79/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2717.4861 \n",
      "Epoch 80/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2773.1838 \n",
      "Epoch 81/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2810.3345\n",
      "Epoch 82/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2809.3784\n",
      "Epoch 83/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2783.0510\n",
      "Epoch 84/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2753.5898\n",
      "Epoch 85/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2801.1687\n",
      "Epoch 86/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2430.0139\n",
      "Epoch 87/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2748.4077\n",
      "Epoch 88/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2724.7612\n",
      "Epoch 89/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2604.6724\n",
      "Epoch 90/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2589.3315\n",
      "Epoch 91/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2734.0103\n",
      "Epoch 92/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2499.9739\n",
      "Epoch 93/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2639.6970\n",
      "Epoch 94/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2582.2412\n",
      "Epoch 95/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2724.4819\n",
      "Epoch 96/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2620.9697\n",
      "Epoch 97/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2622.4038\n",
      "Epoch 98/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2834.2532\n",
      "Epoch 99/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2751.6216\n",
      "Epoch 100/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2669.0288\n",
      "Epoch 101/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2700.3367\n",
      "Epoch 102/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2552.9270 \n",
      "Epoch 103/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2619.8342\n",
      "Epoch 104/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2697.1567\n",
      "Epoch 105/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2601.6187 \n",
      "Epoch 106/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2755.7561\n",
      "Epoch 107/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2705.5344\n",
      "Epoch 108/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2689.3887 \n",
      "Epoch 109/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2662.5022\n",
      "Epoch 110/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2651.8652\n",
      "Epoch 111/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2759.4312 \n",
      "Epoch 112/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2593.5659\n",
      "Epoch 113/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2591.2759\n",
      "Epoch 114/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2674.5974  \n",
      "Epoch 115/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2750.2432 \n",
      "Epoch 116/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2630.5374\n",
      "Epoch 117/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2628.5713\n",
      "Epoch 118/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2889.8779\n",
      "Epoch 119/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2522.3850\n",
      "Epoch 120/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2845.6843\n",
      "Epoch 121/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2744.2246\n",
      "Epoch 122/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2704.8381\n",
      "Epoch 123/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2828.9028\n",
      "Epoch 124/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2481.1055\n",
      "Epoch 125/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2841.1536\n",
      "Epoch 126/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2659.8535 \n",
      "Epoch 127/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2712.9788\n",
      "Epoch 128/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2744.3872\n",
      "Epoch 129/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2672.6382\n",
      "Epoch 130/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2526.7947\n",
      "Epoch 131/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2671.4194\n",
      "Epoch 132/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2704.8066\n",
      "Epoch 133/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2700.0215\n",
      "Epoch 134/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2760.2178\n",
      "Epoch 135/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2573.6536\n",
      "Epoch 136/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2628.9453\n",
      "Epoch 137/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2792.4580\n",
      "Epoch 138/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2628.9160\n",
      "Epoch 139/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2712.8333\n",
      "Epoch 140/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2716.3389 \n",
      "Epoch 141/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2741.3396\n",
      "Epoch 142/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2686.8162\n",
      "Epoch 143/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2700.1733\n",
      "Epoch 144/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2653.1765\n",
      "Epoch 145/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2634.5154\n",
      "Epoch 146/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2978.8000 \n",
      "Epoch 147/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2715.4868\n",
      "Epoch 148/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2538.9727\n",
      "Epoch 149/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2777.8176 \n",
      "Epoch 150/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2670.8857 \n",
      "Epoch 151/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2844.3318\n",
      "Epoch 152/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2633.0176\n",
      "Epoch 153/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2727.9119 \n",
      "Epoch 154/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2710.2063 \n",
      "Epoch 155/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2768.0828\n",
      "Epoch 156/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2703.3472\n",
      "Epoch 157/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2581.4077 \n",
      "Epoch 158/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2657.4111 \n",
      "Epoch 159/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2472.8909\n",
      "Epoch 160/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2707.3818\n",
      "Epoch 161/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2680.9597 \n",
      "Epoch 162/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2748.4246\n",
      "Epoch 163/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2696.9758\n",
      "Epoch 164/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2676.4055 \n",
      "Epoch 165/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2661.4780 \n",
      "Epoch 166/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2571.6716\n",
      "Epoch 167/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2615.1689\n",
      "Epoch 168/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2579.9656 \n",
      "Epoch 169/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2730.3757 \n",
      "Epoch 170/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2646.0029 \n",
      "Epoch 171/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2618.5745\n",
      "Epoch 172/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2671.6052\n",
      "Epoch 173/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2722.5227 \n",
      "Epoch 174/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2676.2537 \n",
      "Epoch 175/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2789.5066\n",
      "Epoch 176/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2662.0273\n",
      "Epoch 177/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2546.3618 \n",
      "Epoch 178/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2633.9255 \n",
      "Epoch 179/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2724.2468\n",
      "Epoch 180/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2619.6514\n",
      "Epoch 181/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2815.9194 \n",
      "Epoch 182/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2468.9434 \n",
      "Epoch 183/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2717.8591\n",
      "Epoch 184/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2664.9766\n",
      "Epoch 185/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2703.0000\n",
      "Epoch 186/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2566.2085\n",
      "Epoch 187/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2764.0220 \n",
      "Epoch 188/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2720.5361 \n",
      "Epoch 189/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2609.8267\n",
      "Epoch 190/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2600.6875  \n",
      "Epoch 191/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2677.6711 \n",
      "Epoch 192/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2579.4480\n",
      "Epoch 193/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2530.9763\n",
      "Epoch 194/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2485.2854 \n",
      "Epoch 195/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2654.7939 \n",
      "Epoch 196/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2685.5298\n",
      "Epoch 197/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2699.8811\n",
      "Epoch 198/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2685.5295\n",
      "Epoch 199/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2598.7542\n",
      "Epoch 200/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2524.8860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2270537a150>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the final model using the best parameters\n",
    "final_model = create_model(learning_rate=best_params['learning_rate'])\n",
    " \n",
    "# Fit the final model using the entire dataset and the best batch_size and epochs\n",
    "final_model.fit(X_scaled, y, batch_size=best_params['batch_size'], epochs=best_params['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d5f52d51-bb61-4daf-af4a-39164c12ef02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1950.8011 \n",
      "Test loss: 1881.4302978515625\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the final model on a test set (assuming you have test data)\n",
    "test_loss = final_model.evaluate(X_scaled, y)\n",
    "print(\"Test loss:\", test_loss)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "63a79be8-3e43-415b-a9bd-354a1c55a7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "            price\n",
      "0    17517.896484\n",
      "1    15889.663086\n",
      "2    21938.046875\n",
      "3    14278.369141\n",
      "4     5364.213379\n",
      "..            ...\n",
      "406  23094.277344\n",
      "407  13176.238281\n",
      "408  13364.712891\n",
      "409  16093.551758\n",
      "410  12776.750977\n",
      "\n",
      "[411 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Predict on new data (assuming X_test_scaled is your new input data)\n",
    "y_predict = final_model.predict(X_test_scaled)\n",
    " \n",
    "# Create a DataFrame for the predictions\n",
    "df_predictions = pd.DataFrame(y_predict, columns=['price'])\n",
    " \n",
    "# Display predictions\n",
    "print(df_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7e71b120-59e2-4860-a12d-b0579c8ae839",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine the predictions with the test data IDs\n",
    "combined_df = pd.concat([test_df['Id'], df_predictions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a7a5b3bc-29ad-4fec-9299-ebcad95f5551",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "combined_df.to_csv('Rant.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d406c7d8-83f3-43dd-8c4b-03c509f1fcae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99ea309-6e40-447e-85c3-771effdb112d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
